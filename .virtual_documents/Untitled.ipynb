











# General purposes libraries
import os, math
from IPython.display import display, Markdown

# Data processing libraries
import numpy as np
import pandas as pd

# Machine learning libraries
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize
from sklearn.cluster import AgglomerativeClustering, KMeans
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline


# Environment
### Random seed to make notebook reproductible
seed = 32

### Special setting to avoid KMEANS memory leak
os.environ["OMP_NUM_THREADS"] = "1"











# Read original data as a data_frame
df_base = pd.read_csv(r".\step1-1_twitter_training.csv", header = None, names=["id", "topic", "sentiment", "tweet"])
init_N = len(df_base)
print(f"Number of rows at file loading: {init_N}")


# Checking for NA values
df_base.isna().sum()

# As they're only located in tweet columns, which is main data column, all na tweet rows are removed.
df_base = df_base[df_base["tweet"].notna()]


# Checking size ranges of messages per sentiment
percentiles = [0, 0.025, 0.05, 0.3, 0.5, 0.7, 0.95, 0.985, 1.0]

desc = (
    df_base.groupby('sentiment')['tweet']
    .apply(lambda s: s.str.len().quantile(percentiles).reset_index(drop=True))
    .unstack()
)
desc.columns = ['min', 'p0.025', 'p0.5', 'p30', 'median', 'p70', 'p95', 'p985', 'max']
print(desc)

### Some tweets have unreasonable short lengths.


# Checking for key training column values
df_base['sentiment'].value_counts()

### Training column has an "Irrelevant" tag.





# It seems some messages have exagerated small length
txts = df_base.loc[df_base['tweet'].str.len() < 4, 'tweet'].sample(n=50, random_state=seed)
display(Markdown("- " + "\n- ".join(txts.tolist())))

# It seems some messages are just whitespaces, so tweet would be stripped
df_base['tweet'] = df_base['tweet'].str.strip()
df_base['tweet'] = df_base['tweet'].replace(r'.*^[^a-zA-Z0-9]+.*$', '#', regex=True)


# It also seems shot length tweets don't bring interesting data (e.g. C, and, to...), and they would be removed.
df_base = df_base[df_base['tweet'].str.len() >= 4]


# Checking cleaned short texts are meaningful:
txts = df_base.loc[df_base['tweet'].str.len() < 10, 'tweet'].sample(n=50, random_state=seed)
display(Markdown("- " + "\n- ".join(txts.tolist())))





# As one modality is flagged "irrelevant", irrelevant lines are highlighted
txts = df_base.loc[df_base['sentiment']=='Irrelevant', 'tweet'].sample(n=50, random_state=seed)
display(Markdown("- " + "\n- ".join(txts.tolist())))

# It seems they're a mix of spam (external url) and misspelled messages (e.g. Friends are upset because the governor has ruled that garden centres cannot open (yet))
# It also seems this category regroups messages from Positive, Negative and Neutral sides.
# As the rest of the dataset seems well tagged and has good Positive / Negative / Neutral proportions, Irrelevant would be removed.
df_base = df_base[df_base['sentiment'] != 'Irrelevant']





# Exporting results to csv
df_base.to_csv(r".\step1-2_cleaned_data.csv", index=False)
print(f"Clean dataset saved: {len(df_base)}/{init_N} data left - {round(100 * len(df_base)/init_N, 2)} %")








# Starting from backup
df_base = pd.read_csv(r".\step1-2_cleaned_data.csv")





# Using a 7 category emotion classifier to create a new "emotion_label" category:
model_name = "j-hartmann/emotion-english-distilroberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, top_k=1)

# Find the right format for output
def predict_emotions(tweet):
    res = classifier(tweet)
    if isinstance(res[0], list):
        return res[0][0]['label']
    else:                         
        return res[0]['label']





# This function is used to apply another function to a dataframe by packs of chunk_size rows
def process_tweets_in_chunks(df: pd.DataFrame,
                             predict_emotions,
                             out_dir,
                             chunk_size: int = 1000,
                             start_chunk: int = 1,
                             out_prefix: str = "df_tagged"):

    # Create output folder if it doesn't exists
    os.makedirs(out_dir, exist_ok=True)

    # Get number of rows and chunks
    n = len(df)
    total_chunks = math.ceil(n / chunk_size)

    # Compute all chunks
    for chunk in range(start_chunk, total_chunks + 1):

        # Chunk row index start from last chunk max index...
        i0 = (chunk - 1) * chunk_size
        # ... and go to current chunk size
        i1 = min(chunk * chunk_size, n)

        # Subset df according to chunk indexes
        part = df.iloc[i0:i1].copy()

        # Apply input function 
        part['emotion_label'] = part['tweet'].apply(predict_emotions)

        # Store result
        fname = os.path.join(out_dir, f"{out_prefix}{chunk:03d}.csv")
        part.to_csv(fname, index=False)

# Apply predict_emotions by chunks of 1000
process_tweets_in_chunks(df_base, predict_emotions, out_dir=r".\outputs", chunk_size=1000, start_chunk=1, out_prefix="df_tagged")





def concat_csv_folder(folder_path: str, output_path: str, pattern: str = ".csv"):
    files = sorted([f for f in os.listdir(folder_path) if f.endswith(pattern)])
    if not files:
        print("Aucun fichier CSV trouvé.")
        return None

    dfs = []
    for i, f in enumerate(files):
        path = os.path.join(folder_path, f)
        df = pd.read_csv(path, header=0)
        dfs.append(df)

    full_df = pd.concat(dfs, ignore_index=True)
    full_df[["id", "topic", "tweet", "sentiment", "emotion_label"]].to_csv(output_path, index=False)
    print(f"→ {len(files)} fichiers fusionnés en : {output_path}")
    return full_df

# Exemple :
tagged_df = concat_csv_folder(r"F:\deeplearningproject\outputs", r"F:\deeplearningproject\step1-3_enriched_data.csv")





# Describing new emotion_label
print(tagged_df['emotion_label'].value_counts())

### It seems that there is quite few disgust and fear in comparaison with the rest


# Describing sentiment x emotion
tagged_df['full_label'] = tagged_df['sentiment']  + "_" + tagged_df['emotion_label']

print(f"Distinct categories: {len(tagged_df['full_label'].value_counts())}")
print("\n")
print(tagged_df['full_label'].value_counts().sort_index())

### It seems all emotions coexist will all sentiments


# Looking at some interesting mixes: 4 not congruent and 2 congruent
n = 5
seed = 0

txts = []

for label in [
    "Negative_joy",
    "Positive_anger",
    "Positive_fear",
    "Positive_sadness",
    "Negative_anger",
    "Positive_joy"
]:
    txts.append(f"### **{label}**")
    samples = "- " + tagged_df.loc[tagged_df['full_label'] == label, 'tweet'].sample(n=n, random_state=seed)
    txts.extend(samples.tolist())

display(Markdown("\n".join(map(str, txts))))


# Non congruent labels somehow capture nuances intexts :
# - Positive anger has things like 'Im getting everything this year. Ps5 headphones and remote fuck it', partly aggresive partly positive
# - Positive sadness has items combining affirmations like "Criminally underrated."

# Congruent labels tend to indicate more simple messages
# - Negative anger has items like "I swear to God I will burn something"
# - Positive joy has items like "Ok I am LOVING THIS"





tagged_df.to_csv(r".\df_tagged_step3.csv", index=False)








dim_df = pd.read_csv(r".\step1-3_enriched_data.csv")


# Reformat full_labels to create a separate class list with text and a Y column with values
Y = pd.get_dummies(dim_df[['sentiment', 'emotion_label', 'full_label']]).astype(float)
label_names = Y.columns.tolist()


# Reduce dimension from 21 initial categories to emb_dim
emb_dim = 5

svd = TruncatedSVD(n_components=emb_dim, random_state=0)
svd.fit(Y)
label_emb = normalize((svd.components_.T * svd.singular_values_))


# 4) Regrouper les labels en 5 paquets par proximité (KMeans sur label_emb)

clu = AgglomerativeClustering(n_clusters=emb_dim, linkage='average', metric='cosine')
label_cluster_id = clu.fit_predict(label_emb)


# --- 4) Optionnel: remettre l'embedding tweet dans le DataFrame ---
cluster_to_labels = {c: [] for c in range(emb_dim)}
for name, cid in zip(label_names, label_cluster_id):
    cluster_to_labels[cid].append(name)
cluster_names = {cid: "+".join(sorted(v)) for cid, v in cluster_to_labels.items()}


cluster_names


cluster_labels = {
    0: "surprised_positivity",
    1: "negative_tension",
    2: "emotional_neutrality",
    3: "mixed_neutrality",
    4: "joyful_positivity"
}


# 1) Construire un mapping label → cluster_id
label_to_cluster = {}
for cid, labels in cluster_to_labels.items():
    for label in labels:
        label_to_cluster[label] = cid

# 2) Associer chaque tweet à son cluster principal
def get_tweet_cluster(row):
    labs = [f"full_label_{row['full_label']}",
            f"emotion_label_{row['emotion_label']}",
            f"sentiment_{row['sentiment']}"]
    ids = [label_to_cluster.get(l) for l in labs if l in label_to_cluster]
    return ids[0] if ids else None  # premier cluster trouvé

dim_df["cluster_id"] = dim_df.apply(get_tweet_cluster, axis=1)
dim_df["cluster"] = dim_df["cluster_id"].map(cluster_labels)


n = 10

txts = []
seed=17

labels = list(cluster_labels.values())

for label in labels:
    txts.append(f"### **{label}**")
    samples = "- " + dim_df.loc[dim_df['cluster'] == label, 'tweet'].sample(n=n, random_state=seed)
    txts.extend(samples.tolist())

display(Markdown("\n".join(map(str, txts))))

# It seems some 



